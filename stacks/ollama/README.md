# Ollama Stack

Ollama downloads large language models and runs them locally. This stack provides a simple way to set up Ollama with Docker.

# Usage

either run

```bash
export USER_HOME=$HOME
```

or edit the .env file so that it machines you user home directory

then run

```bash
docker compose up -d
```

---

# Accessing the Ollama UI

You can access the Ollama UI by navigating to `http://localhost:3000` in your web browser.

---
